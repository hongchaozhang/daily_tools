P1 第一课01 - 1.0欢迎 05:33  
P2 02 - 1.1什么是神经网络 07:17  
P3 03 - 1.2监督学习与神经网络 08:30  
P4 04 - 1.3监督学习与神经网络 10:22  
P5 05 - 1.4关于这门课 02:28  
P6 06 - 1.5课程资源 01:56  
P7 07 - 1.6Geoffrey Hinton访谈（选修） 40:23  
P8 08 - 2.1二元分类 08:24  
P9 09 - 2.2逻辑回归 06:00  
P10 10 - 2.3逻辑回归损失函数 08:12  
P11 11 - 2.4梯度下降 11:24  
P12 12 - 2.5导数 07:11  
P13 13 - 2.6更多导数示例 10:28  
P14 14 - 2.7计算图 03:34  
P15 15 - 2.8计算图求导数 14:35  
P16 16 - 2.9逻辑回归梯度下降 06:43  
P17 17 - 2.10m示例上的梯度下降 08:01  
P18 18 - 2.11向量化 08:05  
P19 19 - 2.12更多向量化示例 06:20  
P20 20 - 2.13向量化逻辑回归 07:33  
P21 21 - 2.14向量化逻辑回归的梯度输出 09:38  
P22 22 - 2.15 Python中的广播 11:07  
P23 23 - 2.16 Python numPy 向量的注释 06:50  
P24 24 - 2.17JuPyter iPython 笔记本的快速浏览 03:44  
P25 25 - 2.18逻辑回归损失函数的解释（选修） 07:15  
P26 26 - 2.19 Pieter Abbeel访谈（选修） 16:04  
P27 27 - 3.1神经网络概览 04:27  
P28 28 - 3.2神经网络的表现形式 05:15  
P29 29 - 3.3计算神经网络的输出 09:59  
P30 30 - 3.4多样本向量化 09:06  
P31 31 - 3.5向量化实现的解释 07:38  
P32 32 - 3.6激活函数 10:57  
P33 33 - 3.7为什么需要非线性激活函数 05:37  
P34 34 - 3.8激活函数的导数 07:58  
P35 35 - 3.9神经网络的梯度下降 09:58  
P36 36 - 3.10反向传播的直觉（选修） 15:49  
P37 37 - 3.11随机初始化 07:58  
P38 38 - 3.12Ian Goodfellow访谈（选修） 14:56  
P39 39 - 4.1深L层神经网络 05:52  
P40 40 - 4.2深层网络中的正向传播 07:16  
P41 41 - 4.3正确的矩阵维数 11:11  
P42 42 - 4.4为什么深度这么有理 10:34  
P43 43 - 4.5为深层神经网络构建模块 08:34  
P44 44 - 4.6正向和反向传播 10:30  
P45 45 - 4.7参数vs超参数 07:18  
P46 46 - 4.8这与大脑的关系是什么 03:18  
P47 第二课1.1训练 开发 测试集 12:05  
P48 1.2偏见 方差 08:47  
P49 1.3机器学习的基本配方 06:22  
P50 1.4正则化 09:43  
P51 1.5为什么正则化可以减少过拟合 07:10  
P52 1.6正规化抛弃 09:26  
P53 1.7理解抛弃 07:05  
P54 1.8其他的正则化方法 08:25  
P55 1.9归一化输入 05:31  
P56 1.10梯度消失 爆炸 06:08  
P57 1.11深度网络权值初始化 06:13  
P58 1.12梯度的数值近似 06:36  
P59 1.13梯度检查 06:35  
P60 1.14梯度检查实施须知 05:19  
P61 2.1小批量梯度下降 11:29  
P62 2.2理解小批量梯度下降 11:19  
P63 2.3指数加权平均 05:59  
P64 2.4理解指数加权平均 09:43  
P65 2.5指数加权平均数的偏差修正 04:12  
P66 2.6动量梯度下降 09:21  
P67 2.7RMSProP 07:42  
P68 2.8适应性矩估计（Adam）算法优化 07:08  
P69 2.9学习速率衰减 06:45  
P70 2.10局部最优解问题 05:24  
P71 3.1参数调整过程 07:11  
P72 3.2使用适当的标准来选择超参数 08:51  
P73 3.3实践中的超参数调整 熊猫vs鱼子酱 06:52  
P74 3.4网络中的正常化激活 08:56  
P75 3.5将Batch Norm拟合到神经网络中 12:56  
P76 3.6为什么Batch Norm有效 11:40  
P77 3.7测试时的Batch Norm 05:47  
P78 3.8Softmax回归 11:48  
P79 3.9训练一个softmax分类器 10:08  
P80 3.10深度学习框架 04:16  
P81 3.11TensorFlow 15:02  
P82 第三课01 - 1.1为什么选择ML策略 02:43  
P83 02 - 1.2正交化 10:39  
P84 03 - 1.3单数评价指标 07:17  
P85 04 - 1.4满足和优化指标 05:59  
P86 05 - 1.5训练 开发 测试分布 06:36  
P87 06 - 1.6开发和测试集的大小和指标 05:40  
P88 07 - 1.7何时更改开发 测试集和指标 11:08  
P89 08 - 1.8为什么选择人类水平表现 05:47  
P90 09 - 1.9可避免的偏见 07:00  
P91 10 - 1.10理解人类水平表现 11:13  
P92 11 - 1.11超越人类水平表现 06:22  
P93 12 - 1.12提高您的模型性能 04:37  
P94 13 - 1.13Andrej Kar Pathy访谈 15:11  
P95 14 - 2.1进行误差分析 10:33  
P96 15 - 2.2清理错误标注的数据 13:06  
P97 16 - 2.3快速构建您的第一个系统，并进行迭代 05:26  
P98 17 - 2.4训练和测试的不同分布 10:56  
P99 18 - 2.5不匹配数据分布的偏差和方差 18:17  
P100 19 - 2.6解决数据不匹配问题 10:09  
P101 20 - 2.7迁移学习 11:18  
P102 21 - 2.8多任务学习 13:00  
P103 22 - 2.9什么是端到端深度学习 11:48  
P104 23 - 2.10是否使用端到端深度学习 10:20  
P105 24 - 2.11Ruslan Salakhutdinov访谈 17:09  
P106 第四课01 - 1.1计算机视觉 05:45  
P107 02 - 1.2边缘探测示例 11:31  
P108 03 - 1.3更多边缘探测 07:58  
P109 04 - 1.4填充 09:50  
P110 05 - 1.5卷积步长 09:02  
P111 06 - 1.6三维卷积 10:45  
P112 07 - 1.7卷积网络的一层 16:11  
P113 08 - 1.8卷积网络的简单示例 08:33  
P114 09 - 1.9池化层 10:26  
P115 10 - 1.10CNN示例 12:38  
P116 11 - 1.11为什么用卷积 09:41  
P117 12 - 2.1为什么要进行案例研究？ 03:09  
P118 13 - 2.2经典网络 18:20  
P119 14 - 2.3残差网络 07:09  
P120 15 - 2.4为什么使用残差网络 09:13  
P121 16 - 2.5网络中的网络及1x1卷积 06:41  
P122 17 - 2.6初始网络动机 10:15  
P123 18 - 2.7初始网络 08:47  
P124 19 - 2.8MobileNet 16:19  
P125 20 - 2.9MobileNet架构 08:33  
P126 21 - 2.10EfficientNet 03:40  
P127 22 - 2.11使用开放源码 04:57  
P128 23 - 2.12迁移学习 08:49  
P129 24 - 2.13数据增强 09:32  
P130 25 - 2.14计算机视觉状态 12:39  
P131 26 - 3.1目标定位 11:55  
P132 27 - 3.2地标检测 05:57  
P133 28 - 3.3目标检测 05:50  
P134 29 - 3.4在卷积网络上实现滑动窗口 11:09  
P135 30 - 3.5边界框预测 14:32  
P136 31 - 3.6并交比 04:19  
P137 32 - 3.7非极大值抑制 08:03  
P138 33 - 3.8锚框 09:44  
P139 34 - 3.9YOLO（You Only Look Once）算法 07:02  
P140 35 - 3.10区域推荐网络（选修） 06:28  
P141 3.11 用u-net进行语义分割 07:22  
P142 3.12 转置卷积 07:40  
P143 3.13 u-net 结构灵感 03:22  
P144 3.14 u-net 结构 07:42  
P145 36 - 4.1.1什么是人脸识别 04:38  
P146 37 - 4.1.2单样本学习 04:46  
P147 38 - 4.1.3孪生神经网络 04:52  
P148 39 - 4.1.4TriPlet Loss三元组损失 15:31  
P149 40 - 4.1.5人脸验证和二进制分类 06:06  
P150 41 - 4.2.1什么是神经风格迁移 02:03  
P151 42 - 4.2.2什么是深度卷积神经网络学习 07:58  
P152 43 - 4.2.3代价函数 04:00  
P153 44 - 4.2.4内容代价函数 03:38  
P154 45 - 4.2.5风格代价函数 13:18  
P155 46 - 4.2.6 1维和3维推广 09:09  
P156 第五课1.1为什么用序列模型 03:01  
P157 1.2注释 09:16  
P158 1.3循环神经网络模型 16:32  
P159 1.4通过时间的反向传播 06:12  
P160 1.5不同类型的RNNs 09:35  
P161 1.6语言模型和序列生成 12:02  
P162 1.7对新序列的采样 08:39  
P163 1.8RNNs的梯度消失 06:29  
P164 1.9门控循环单元（GRU） 17:07  
P165 1.10长短期记忆（LSTM） 09:54  
P166 1.11双向RNN 08:20  
P167 1.12深度RNNs 05:17  
P168 2.1词表示 10:08  
P169 2.2使用词嵌入 09:23  
P170 2.3词嵌入的性能 11:55  
P171 2.4矩阵嵌入 05:58  
P172 2.5学习词嵌入 10:09  
P173 2.6词转换成向量形式 12:48  
P174 2.7负采样 11:54  
P175 2.8GloVe 词向量 11:09  
P176 2.9情感分类 07:38  
P177 2.10词嵌入除偏 11:09  
P178 3.1基础模型 06:19  
P179 3.2选择最有可能的句子 08:57  
P180 3.3集束搜索 11:55  
P181 3.4细化集束搜索 11:01  
P182 3.5集束搜索中的错误分析 09:44  
P183 3.6Bleu分数（选修） 16:27  
P184 3.7注意力模型直觉 09:42  
P185 3.8注意力模型 12:23  
P186 3.9语音识别 08:54  
P187 3.10触发词检测 05:04  
P188 3.11Transformer网络直觉 05:30  
P189 3.12自注意力机制 11:44  
P190 3.13多头注意力机制 08:21  
P191 3.14Transformer网络 13:11  
P192 3.15结论及感谢 02:45